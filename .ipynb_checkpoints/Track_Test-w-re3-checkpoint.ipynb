{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kierandonnelly/anaconda3/envs/thesis_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kierandonnelly/anaconda3/envs/thesis_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kierandonnelly/anaconda3/envs/thesis_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kierandonnelly/anaconda3/envs/thesis_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kierandonnelly/anaconda3/envs/thesis_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kierandonnelly/anaconda3/envs/thesis_env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import time\n",
    "import imutils\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "import cv2\n",
    "from tqdm.auto import trange, tqdm\n",
    "from re3_tensorflow.tracker import re3_tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to implement one tracker on one video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_track(tracker_name, tracker_inst, video_name, video_file, frames_df, resize=0):\n",
    "    '''\n",
    "    Runs a specified tracking algorithm on a specified video \\\n",
    "    file and calls function to calculate evaluation metrics\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    tracker_name (str): short name of tracking algo\n",
    "    tracker_inst: instantiation of tracker class\n",
    "    video_name: (str) short name of video clip\n",
    "    video_file: (str): path to video file\n",
    "    frames_df (pd.DataFrame): DF of ground truth objects in video clip\n",
    "    resize (int): fraction to which video should be resized (1, 2 or 4). 1 = unchanged (Def), 2 = 1/2, 4 = 1/4\n",
    "    '''\n",
    "    \n",
    "    # CONSTANTS\n",
    "    THRESHOLD_IOU, THRESHOLD_DIST = 0.4, 0.1\n",
    "    \n",
    "    # initialise vars\n",
    "    frame_count = frames_df['frame'].min()\n",
    "    fn_count, fp_count, fp_thresh_count, tp_count, tn_count, covered_count = 0, 0, 0, 0, 0, 0\n",
    "    iter_count = 0\n",
    "    tracking_fps = 0\n",
    "    iou_list, dist_list = [], []\n",
    "    tracker = None\n",
    "    writer = None\n",
    "    re3 = False\n",
    "    \n",
    "    # Initialize the video stream and pointer to output video file\n",
    "    vs = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    total_frames = int(vs.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # start clock for FPS\n",
    "    tracker_time = start_timer()\n",
    "    \n",
    "#     while True:\n",
    "    for i in tqdm(range(total_frames), desc=tracker_name + ' progress'):\n",
    "        # drawn rect flag\n",
    "        drawn = False\n",
    "        \n",
    "        # read the next frame from the file\n",
    "        grabbed, frame = vs.read()\n",
    "        \n",
    "        # if no frame grabbed, reached end of clip\n",
    "        if not grabbed:\n",
    "            print ('[INFO] End of clip')\n",
    "            break\n",
    "        \n",
    "        # boolean for whether there is a ground truth object in the frame\n",
    "        object_exists = (frames_df['frame'] == frame_count).any()\n",
    "\n",
    "        if resize is not 0:\n",
    "            frame = cv2.resize(frame, (resize, int(resize*(9/16))))\n",
    "            \n",
    "        # will resize the frame for faster processing\n",
    "        new_width = frame.shape[1]\n",
    "        new_height = frame.shape[0]\n",
    "        \n",
    "        # number of rows in frames determine number of objects, N_objects\n",
    "        N_objects = len(frames_df.index)\n",
    "        \n",
    "        # bb for ground truth obj (relative values)\n",
    "        gt_bb = (frames_df.loc[frames_df['frame'] == frame_count].squeeze()[1:])*(new_width, new_height, new_width, new_height) if object_exists else 0\n",
    "        \n",
    "        # initialize the writer\n",
    "        if writer is None:\n",
    "            print()\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            fps = vs.get(5)\n",
    "            writer = cv2.VideoWriter('./output/' + video_name + '_' + tracker_name + \\\n",
    "                                     '_' + str(new_width) + '.mp4', fourcc, fps, \\\n",
    "                                     (frame.shape[1], frame.shape[0]), True)\n",
    "        \n",
    "        if tracker is None:\n",
    "            # create tracker object\n",
    "            tracker = tracker_inst\n",
    "            \n",
    "            # initialise with bounding box\n",
    "            if tracker_name == 're3': # special case for re3\n",
    "                # NB: format is (x1, y1, x2, y2) for re3\n",
    "                re3 = True\n",
    "                tracker.track('video', frame[:,:,::-1], \\\n",
    "                                                    (gt_bb[0], gt_bb[1], gt_bb[0] + gt_bb[2], gt_bb[1] + gt_bb[3]))\n",
    "            \n",
    "            else: # normal case for opencv trackers\n",
    "                tracker.init(frame, tuple(gt_bb))\n",
    "            \n",
    "            # first frame given, so always draw it\n",
    "            drawn = draw_rect(frame, gt_bb, frame_count, drawn, False)\n",
    "            \n",
    "        if tracker is not None:\n",
    "            # update the tracker and grab the tracked object\n",
    "            \n",
    "            if re3 == True:\n",
    "                tracking, trk_bb = tracker.track('video', frame[:,:,::-1])\n",
    "                # convert to (x1, y1, x2, y2)\n",
    "                trk_bb = (trk_bb[0], trk_bb[1], trk_bb[2] - trk_bb[0], trk_bb[3] - trk_bb[1])\n",
    "            else:\n",
    "                tracking, trk_bb = tracker.update(frame)\n",
    "\n",
    "            # will need modifications for trackers outside of OpenCV-8\n",
    "            if tracking is False:\n",
    "                if ~object_exists:\n",
    "                    # true negative\n",
    "                    tn_count += 1\n",
    "                    \n",
    "                elif object_exists:\n",
    "                    # false negative\n",
    "                    fn_count += 1\n",
    "\n",
    "            elif tracking:\n",
    "                if object_exists:\n",
    "                    iou_t, dist_rel_t = compute_iou_dist(gt_bb, trk_bb, new_width, new_height) # pass them in (x1, y1, w, h) format\n",
    "                    \n",
    "                    # completeness metrics\n",
    "                    if iou_t >= THRESHOLD_IOU:\n",
    "                        covered_count += 1\n",
    "                    \n",
    "                    if (dist_rel_t < THRESHOLD_DIST):\n",
    "                        # true positive\n",
    "                        tp_count += 1\n",
    "                        \n",
    "                    elif (dist_rel_t >= THRESHOLD_DIST):\n",
    "                        # false positive\n",
    "                        fp_thresh_count += 1\n",
    "                        \n",
    "                    iou_list.append(iou_t)\n",
    "                    dist_list.append(dist_rel_t)\n",
    "                    \n",
    "                elif ~object_exists:\n",
    "                    # false positive\n",
    "                    fp_count += 1\n",
    "                \n",
    "                # tracking taken place, so draw frame\n",
    "                drawn = draw_rect(frame, trk_bb, frame_count, drawn, re3)\n",
    "        \n",
    "        # increment frame counter\n",
    "        frame_count += 1\n",
    "        \n",
    "        # increment loop counter\n",
    "        iter_count += 1\n",
    "        \n",
    "        # write the frame to disk\n",
    "        if writer is not None:\n",
    "            writer.write(frame)\n",
    "    \n",
    "    # finalise FPS\n",
    "    tracking_fps = stop_and_report(frame_count, tracker_time)\n",
    "    \n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "        \n",
    "    # call metrics function - returns dictionary of metrics\n",
    "    metrics = compute_metrics(fp_thresh_count, fp_count, tp_count, tn_count, fn_count, \\\n",
    "                              covered_count, dist_list, iou_list, N_objects, frame_count, \\\n",
    "                              tracking_fps, clip_name, str(new_width), tracker_name, \\\n",
    "                              THRESHOLD_IOU, THRESHOLD_DIST)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rect(frame, box, frame_count, drawn, re3):\n",
    "    p1 = (int(box[0]), int(box[1]))\n",
    "    p2 = (int(box[0] + box[2]), int(box[1] + box[3])) # if re3 != True else (int(box[2]), int(box[3]))\n",
    "    if drawn != True:\n",
    "        cv2.rectangle(frame, p1, p2, (255, 0, 0), 2, 1)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_timer():\n",
    "    return time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_and_report(frames, start_time):\n",
    "    return frames/(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "This section defines functions to compute the various evaluation metrics detailed in Chapter 3 in the report. The metrics to compute are as follows:\n",
    "- Recall: Correctly matched detections as proportion of total ground truth objects (of a sequence).\n",
    "- Precision/N-SODA: Correctly matched detections as a proportion of total detections (of a sequence).\n",
    "- FAF: Number of false alarms (incorrect detections) per frame averaged over a sequence.\n",
    "- SODP: Average overlap between ground truth and system output.\n",
    "- SOTA: Combines false negatives and false positives without weighting factors.\n",
    "- SOTP: Average distance between centroids of ground truth and system output.\n",
    "- TDE: Distance beetween the ground-truth annotation and the tracking result.\n",
    "- MT: The ground-truth trajectory is covered by the tracker output for more than 80% of its length.\n",
    "- ML: The ground-truth trajectory is covered by the tracker output for less than 20% of its length.\n",
    "- PT: The ground-truth trajectory is covered by the tracker output for between 20% and 80% of its length.\n",
    "- FM: Number of times that a ground-truth trajectory is interrupted in the tracking result, normalised over sequence.\n",
    "- RS: Ratio of tracks which correctly recover from short term occlusion.\n",
    "- RL: Ratio of tracks which correctly recover from long term occlusion.\n",
    "\n",
    "### Terminology\n",
    "- **True Positive**: Distance between centroids in both x and y directions below distance threshold.\n",
    "- **True Negative**: No object and no hypothesis.\n",
    "- **False Negative**: No hypothesis from tracker but object exists.\n",
    "- **False Positive**: Hypothesis but no object exists.\n",
    "- **False Positive - Exceeds Threshold**: Distance between centroids in both x and y directions exceeds distance threshold.\n",
    "- **Distance**: Euclidean distance between centroids of tracker output and ground truth.\n",
    "- **Covered**: Overlap exceeds THRESHOLD_IOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Compute Each Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection over Union (IoU) and Distance Between Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou_dist(box_a, box_b, frame_width, frame_height):\n",
    "    \n",
    "    # compute centroids (x1 + half width, y1 + half height)\n",
    "    centroid_a = (box_a[0] + 0.5*box_a[2], box_a[1] + 0.5*box_a[3])\n",
    "    centroid_b = (box_b[0] + 0.5*box_b[2], box_b[1] + 0.5*box_b[3])\n",
    "    \n",
    "    # (x1, y1, x1+w, y1+h) -> (x1, y1, x2, y2)\n",
    "    box_a = (box_a[0], box_a[1], box_a[0] + box_a[2], box_a[1] + box_a[3])\n",
    "    box_b = (box_b[0], box_b[1], box_b[0] + box_b[2], box_b[1] + box_b[3])\n",
    "    \n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    x_a = max(box_a[0], box_b[0])\n",
    "    y_a = max(box_a[1], box_b[1])\n",
    "    x_b = min(box_a[2], box_b[2])\n",
    "    y_b = min(box_a[3], box_b[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    area_overlap = max(0, x_b - x_a + 1) * max(0, y_b - y_a + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth rectangles\n",
    "    area_box_a = (box_a[2] - box_a[0] + 1) * (box_a[3] - box_a[1] + 1)\n",
    "    area_box_b = (box_b[2] - box_b[0] + 1) * (box_b[3] - box_b[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = area_overlap / float(area_box_a + area_box_b - area_overlap)\n",
    "    \n",
    "    dist_x_rel = (centroid_b[0] - centroid_a[0])/frame_width\n",
    "    dist_y_rel = (centroid_b[1] - centroid_a[1])/frame_height\n",
    "    \n",
    "    dist_rel = math.hypot(dist_x_rel, dist_y_rel)\n",
    "    \n",
    "    # distance is hypotenuse of two centroid coordinates\n",
    "    # dist = math.hypot(centroid_b[0] - centroid_a[0], centroid_b[1] - centroid_a[1])\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou, dist_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SOTP - Single Object Tracking Precision, or Average Distance\n",
    "Average distance between boxes only on frames with both an object and a hypothesis, i.e.:\n",
    "- `fp_thresh_count`\n",
    "- `tp_count`\n",
    "\n",
    "`dist_t` should be forced to zero for frames with any results outside of the above (fp, tn, fn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_sotp(dist_list, N_match):\n",
    "    total_dist = sum(dist_list)\n",
    "    return (total_dist/N_match) if N_match != 0 else np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_sota(fp_thresh_count, fp_count, fn_count, N_objects):\n",
    "    return 1 - (fp_thresh_count + fn_count)/(N_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_recall(tp_count, N_objects):\n",
    "    return tp_count/N_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_precision(tp_count, fp_thresh_count, fp_count):\n",
    "    precision = tp_count/(tp_count + fp_count + fp_thresh_count) if tp_count != 0 else 0\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. FPF, False Positives per Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fpf(fp_count, fp_thresh_count, N_frames):\n",
    "    return (fp_count + fp_thresh_count)/N_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. SODP, Single Object Detection Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_sodp(iou_list, N_match):\n",
    "    total_iou = sum(iou_list)\n",
    "    sodp = total_iou/N_match if N_match != 0 else 0\n",
    "    return sodp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_mt(completeness):\n",
    "    return 1 if completeness >= 0.8 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_pt(completeness):\n",
    "    return 1 if (completeness > 0.2) and (completeness < 0.8) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_ml(completeness):\n",
    "    return 1 if completeness <= 0.2 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. FM, Fragmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fm(fp_thresh_count, fp_count, fn_count, N_objects):\n",
    "    return (fp_thresh_count + fn_count)/(N_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Function\n",
    "This function calculates all metrics and returns them in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(fp_thresh_count, fp_count, tp_count, tn_count, fn_count, \\\n",
    "                    covered_count, dist_list, iou_list, N_objects, N_frames, \\\n",
    "                    tracking_fps, clip_name, width, tracker_name, THRESHOLD_IOU,\\\n",
    "                    THRESHOLD_DIST):\n",
    "    \n",
    "    N_match = fp_thresh_count + tp_count\n",
    "    completeness = covered_count/N_match if N_match != 0 else 0\n",
    "    \n",
    "    sotp = metric_sotp(dist_list, N_match)\n",
    "    sota = metric_sota(fp_thresh_count, fp_count, fn_count, N_objects)\n",
    "    recall = metric_recall(tp_count, N_objects)\n",
    "    precision = metric_precision(tp_count, fp_thresh_count, fp_count)\n",
    "    fpf = metric_fpf(fp_count, fp_thresh_count, N_frames)\n",
    "    sodp = metric_sodp(iou_list, N_match)\n",
    "    mt = metric_mt(completeness)\n",
    "    pt = metric_pt(completeness)\n",
    "    ml = metric_ml(completeness)\n",
    "    fm = metric_fm(fp_thresh_count, fp_count, fn_count, N_objects)\n",
    "    \n",
    "    results_dict = {'Clip': clip_name,\n",
    "                    'Res': width,\n",
    "                    'Tracker': tracker_name,\n",
    "                    'IOU_Thresh': THRESHOLD_IOU,\n",
    "                    'Dist_Thresh': THRESHOLD_DIST,\n",
    "                    'SOTP': np.round(sotp,4),\n",
    "                    'SOTA': np.round(sota,4),\n",
    "                    'Recall': np.round(recall,4),\n",
    "                    'Precision': np.round(precision,4),\n",
    "                    'FPF': np.round(fpf, 4),\n",
    "                    'SODP': np.round(sodp,4),\n",
    "                    'MT': mt,\n",
    "                    'PT': pt,\n",
    "                    'ML': ml,\n",
    "                    'Completeness': np.round(completeness,2),\n",
    "                    'FM': np.round(fm,2),\n",
    "                    'FPS': np.round(tracking_fps,2),\n",
    "                    'fp_count': fp_count,\n",
    "                    'fp_thresh_count': fp_thresh_count,\n",
    "                    'tp_count': tp_count,\n",
    "                    'fn_count': fn_count,\n",
    "                    'tn_count': tn_count,\n",
    "                    'N_objects': N_objects,\n",
    "                    'N_frames': N_frames,\n",
    "                    'N_match': N_match}\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of trackers to run through for each video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/kierandonnelly/thesis/re3_tensorflow/logs/checkpoints/model.ckpt-260946\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "trackers = {\n",
    "            're3': re3_tracker.Re3Tracker()}\n",
    "#             'mil': cv2.TrackerMIL_create()}\n",
    "#             'boosting': cv2.TrackerBoosting_create(),\n",
    "#             'kcf': cv2.TrackerKCF_create(),\n",
    "#             'tld': cv2.TrackerTLD_create(),\n",
    "#             'medianflow': cv2.TrackerMedianFlow_create(),\n",
    "#             'goturn': cv2.TrackerGOTURN_create(),\n",
    "#             'mosse': cv2.TrackerMOSSE_create(),\n",
    "#             'csrt': cv2.TrackerCSRT_create()}\n",
    "#                 # 'pysot': base_tracker.BaseTracker()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clips = {'Bike02': '/Users/kierandonnelly/thesis/raw_clips/Bike02.mp4'}\n",
    "# clips = {'Bike03': '/Users/kierandonnelly/thesis/raw_clips/Bike03.mp4'}\n",
    "# clips = {'Bike04': '/Users/kierandonnelly/thesis/raw_clips/Bike04.mp4'}\n",
    "# clips = {'Bike05': '/Users/kierandonnelly/thesis/raw_clips/Bike05.mp4'}\n",
    "# clips = {'Bike07': '/Users/kierandonnelly/thesis/raw_clips/Bike07.mp4'}\n",
    "# clips = {'Bike08': '/Users/kierandonnelly/thesis/raw_clips/Bike08.mp4'}\n",
    "# clips = {'Bike09': '/Users/kierandonnelly/thesis/raw_clips/Bike09.mp4'}\n",
    "# clips = {'Ski01': '/Users/kierandonnelly/thesis/raw_clips/Ski01.mp4'}\n",
    "# clips = {'Ski02': '/Users/kierandonnelly/thesis/raw_clips/Ski02.mp4'}\n",
    "# clips = {'Ski03': '/Users/kierandonnelly/thesis/raw_clips/Ski03.mp4'}\n",
    "# clips = {'Ski04': '/Users/kierandonnelly/thesis/raw_clips/Ski04.mp4'}\n",
    "# clips = {'Snowboard01': '/Users/kierandonnelly/thesis/raw_clips/Snowboard01.mp4'}\n",
    "# clips = {'Sup01': '/Users/kierandonnelly/thesis/raw_clips/Sup01.mp4'}\n",
    "# clips = {'Surf01': '/Users/kierandonnelly/thesis/raw_clips/Surf01.mp4'}\n",
    "# clips = {'Wake01': '/Users/kierandonnelly/thesis/raw_clips/Wake01.mp4'}\n",
    "# clips = {'Car01': '/Users/kierandonnelly/thesis/raw_clips/Car01.mp4'}\n",
    "# clips = {'Human01': '/Users/kierandonnelly/thesis/raw_clips/Human01.mp4'}\n",
    "# clips = {'Slalom01': '/Users/kierandonnelly/thesis/raw_clips/Slalom01.mp4'}\n",
    "\n",
    "clips = {'Bike02': '/Users/kierandonnelly/thesis/raw_clips/Bike02.mp4',\n",
    "         'Bike03': '/Users/kierandonnelly/thesis/raw_clips/Bike03.mp4',\n",
    "         'Bike04': '/Users/kierandonnelly/thesis/raw_clips/Bike04.mp4',\n",
    "         'Bike05': '/Users/kierandonnelly/thesis/raw_clips/Bike05.mp4',\n",
    "         'Bike07': '/Users/kierandonnelly/thesis/raw_clips/Bike07.mp4',\n",
    "         'Bike08': '/Users/kierandonnelly/thesis/raw_clips/Bike08.mp4',\n",
    "         'Bike09': '/Users/kierandonnelly/thesis/raw_clips/Bike09.mp4',\n",
    "         'Ski01': '/Users/kierandonnelly/thesis/raw_clips/Ski01.mp4',\n",
    "         'Ski02': '/Users/kierandonnelly/thesis/raw_clips/Ski02.mp4',\n",
    "         'Ski03': '/Users/kierandonnelly/thesis/raw_clips/Ski03.mp4',\n",
    "         'Ski04': '/Users/kierandonnelly/thesis/raw_clips/Ski04.mp4',\n",
    "         'Snowboard01': '/Users/kierandonnelly/thesis/raw_clips/Snowboard01.mp4',\n",
    "         'Sup01': '/Users/kierandonnelly/thesis/raw_clips/Sup01.mp4',\n",
    "         'Surf01': '/Users/kierandonnelly/thesis/raw_clips/Surf01.mp4',\n",
    "         'Wake01': '/Users/kierandonnelly/thesis/raw_clips/Wake01.mp4',\n",
    "         'Car01': '/Users/kierandonnelly/thesis/raw_clips/Car01.mp4',\n",
    "         'Human01': '/Users/kierandonnelly/thesis/raw_clips/Human01.mp4',\n",
    "         'Slalom01': '/Users/kierandonnelly/thesis/raw_clips/Slalom01.mp4'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over videos in directory and run tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccef1ce48d144e2b18d188b8783ed0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Bike02 progress: ', max=1, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5346edf4274f0c9f046ae1732a5a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='re3 progress', max=98, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current tracking speed:   4.993 FPS\n",
      "Current image read speed: 117.468 FPS\n",
      "Mean tracking speed:      0.000 FPS\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6104988028412da153627c835cbdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Bike03 progress: ', max=1, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7dcaf5070044d6ba8581c1f28a8ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='re3 progress', max=92, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current tracking speed:   18.344 FPS\n",
      "Current image read speed: 134.774 FPS\n",
      "Mean tracking speed:      17.062 FPS\n",
      "\n",
      "[INFO] End of clip\n",
      "\n",
      "[INFO] Finished\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "resize = 0\n",
    "\n",
    "# iterate over clips\n",
    "for clip_name, clip_loc in clips.items():\n",
    "    \n",
    "    # retrieve frame data for clip\n",
    "    frames = pd.read_csv('./frame_data/' + str(clip_name) + '.csv', sep=';')\n",
    "    \n",
    "    # iterate over tracker dictionary\n",
    "    for tracker_name, tracker_inst in tqdm(trackers.items(), desc=clip_name + ' progress: '):\n",
    "\n",
    "            # call tracking function\n",
    "            metrics = run_track(tracker_name, tracker_inst, clip_name, clip_loc, frames, resize)\n",
    "            \n",
    "            metrics_list.append(metrics)\n",
    "            \n",
    "print ('[INFO] Finished')\n",
    "    \n",
    "# convert list of metrics dicts to df\n",
    "results_df = pd.DataFrame(metrics_list, columns=['Clip', 'Res', 'Tracker', 'IOU_Thresh', 'Dist_Thresh', 'SOTP',\\\n",
    "                                                 'SOTA', 'Recall','Precision', 'FPF', 'SODP', 'MT', 'PT', 'ML',\\\n",
    "                                                 'Completeness','FM', 'FPS', 'fp_count', 'fp_thresh_count',\\\n",
    "                                                 'tp_count','fn_count', 'tn_count', 'N_objects', 'N_frames', 'N_match'])\n",
    "\n",
    "res = 'native' if resize == 0 else resize\n",
    "\n",
    "# save to CSV\n",
    "results_df.to_csv('metrics_' + clip_name + '_' + str(res)  + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Thesis Env",
   "language": "python",
   "name": "thesis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
